parameters:
  
  max_num_batched_tokens:
    name: "max-num-batched-tokens"
    enabled: true
    options: [512, 1024, 2048, 4096, 8192, 16384]

  block_size:
    name: "block-size"
    enabled: true
    options: [1, 8, 16, 32, 64, 128]

  max_num_seqs:
    name: "max-num-seqs"
    enabled: true
    options: [64, 128, 192, 256, 384, 512, 1024, 2048, 4096, 8192, 16384]


  gpu_memory_utilization:
    name: "gpu-memory-utilization"
    enabled: true
    range:
      start: 0.90
      end: 0.95
      step: 0.01



  cuda_graph_sizes:
    name: "cuda-graph-sizes"
    enabled: true
    range:
      start: 8
      end: 16328
      step: 64


  long_prefill_token_threshold:
    name: "long-prefill-token-threshold"
    enabled: true
    options: [0, 256, 512, 1024, 2048]

  max_num_partial_prefills:
    name: "max-num-partial-prefills"
    enabled: true
    options: [1, 2, 4, 8]

  max_seq_len_to_capture:
    name: "max-seq-len-to-capture"
    enabled: true
    options: [256, 512, 1024, 2048, 4096, 8192, 16384]

  qps:
    name: "qps"
    enabled: true
    range:
      start: 9
      end: 12
      step: 0.1


optimization:
  # Available approaches:
  # - "single_objective": Maximize throughput only
  # - "multi_objective": Find Pareto-optimal throughput vs latency trade-offs
  approach: "single_objective"
  
  n_trials: 200

  # Choose between ["botorch", "nsga2", "tpe", "random", "grid"]
  # BoTorch is particularly effective for parallel optimization across multiple GPUs
  # as it can suggest multiple candidates simultaneously and learn from parallel trials
  sampler: "tpe"

settings:
  trial_interval: 30