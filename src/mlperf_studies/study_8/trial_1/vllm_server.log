INFO 07-22 15:58:06 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 15:58:13 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 15:58:13 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 15:58:13 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 15:58:14 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 07-22 15:58:14 [cli_args.py:300] non-default args: {'block_size': 16, 'gpu_memory_utilization': 0.9400000000000001, 'kv_cache_dtype': 'fp8', 'max_num_batched_tokens': 16384, 'max_num_partial_prefills': 2, 'cuda_graph_sizes': [776], 'long_prefill_token_threshold': 256, 'enable_chunked_prefill': True, 'disable_log_requests': True}
INFO 07-22 15:58:25 [config.py:793] This model supports multiple tasks: {'score', 'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
WARNING 07-22 15:58:25 [arg_utils.py:1583] --kv-cache-dtype is not supported by the V1 Engine. Falling back to V0. 
INFO 07-22 15:58:25 [config.py:1503] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-22 15:58:25 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 07-22 15:58:25 [config.py:2128] Concurrent partial prefills enabled with max_num_partial_prefills=2, max_long_partial_prefills=1, long_prefill_token_threshold=256
INFO 07-22 15:58:25 [api_server.py:257] Started engine process with PID 210481
INFO 07-22 15:58:28 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 15:58:31 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 15:58:31 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 15:58:31 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 15:58:31 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 256}, use_cached_outputs=True, 
INFO 07-22 15:58:38 [cuda.py:273] Cannot use FlashAttention backend for FP8 KV cache.
WARNING 07-22 15:58:38 [cuda.py:275] Please use FlashInfer backend with FP8 KV Cache for better performance by setting environment variable VLLM_ATTENTION_BACKEND=FLASHINFER
INFO 07-22 15:58:38 [cuda.py:289] Using XFormers backend.
INFO 07-22 15:58:39 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-22 15:58:39 [model_runner.py:1170] Starting to load model meta-llama/Llama-3.1-8B...
INFO 07-22 15:58:40 [weight_utils.py:291] Using model weights format ['*.safetensors']
INFO 07-22 15:58:41 [weight_utils.py:307] Time spent downloading weights for meta-llama/Llama-3.1-8B: 0.551288 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.58it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.11it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.69it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.54it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.72it/s]

INFO 07-22 15:58:44 [default_loader.py:280] Loading weights took 2.38 seconds
INFO 07-22 15:58:44 [model_runner.py:1202] Model loading took 14.9889 GiB and 4.664731 seconds
INFO 07-22 15:58:46 [worker.py:291] Memory profiling takes 1.64 seconds
INFO 07-22 15:58:46 [worker.py:291] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.94) = 41.85GiB
INFO 07-22 15:58:46 [worker.py:291] model weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.70GiB; the rest of the memory reserved for KV Cache is 25.08GiB.
INFO 07-22 15:58:46 [executor_base.py:112] # cuda blocks: 25684, # CPU blocks: 4096
INFO 07-22 15:58:46 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 3.14x
INFO 07-22 15:58:47 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.03it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:14,  2.26it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.29it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:13,  2.35it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:12,  2.41it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:12,  2.36it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:11,  2.39it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:11,  2.45it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:10,  2.39it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:10,  2.43it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:09,  2.46it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:09,  2.44it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:08,  2.48it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:05<00:08,  2.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:08,  2.42it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:06<00:07,  2.45it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:07,  2.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:06,  2.46it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:07<00:06,  2.46it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:08<00:05,  2.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:08<00:05,  2.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:09<00:04,  2.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:10<00:03,  2.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:10<00:03,  2.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:11<00:03,  2.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:11<00:02,  2.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:12<00:01,  2.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:12<00:01,  2.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:12<00:01,  2.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:13<00:00,  2.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.47it/s]
INFO 07-22 15:59:01 [model_runner.py:1670] Graph capturing finished in 14 secs, took 0.26 GiB
INFO 07-22 15:59:01 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 17.42 seconds
WARNING 07-22 15:59:02 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 07-22 15:59:02 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 15:59:03 [serving_completion.py:65] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 15:59:03 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000
INFO 07-22 15:59:03 [launcher.py:28] Available routes are:
INFO 07-22 15:59:03 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /health, Methods: GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /load, Methods: GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /ping, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /ping, Methods: GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /version, Methods: GET
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /pooling, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /classify, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /score, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /rerank, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /invocations, Methods: POST
INFO 07-22 15:59:03 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [210183]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:32820 - "GET /health HTTP/1.1" 200 OK
INFO 07-22 16:02:05 [metrics.py:486] Avg prompt throughput: 451.8 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 90 reqs, Swapped: 0 reqs, Pending: 13268 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:10 [metrics.py:486] Avg prompt throughput: 11125.6 tokens/s, Avg generation throughput: 421.6 tokens/s, Running: 120 reqs, Swapped: 0 reqs, Pending: 13147 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:15 [metrics.py:486] Avg prompt throughput: 8430.7 tokens/s, Avg generation throughput: 1020.6 tokens/s, Running: 123 reqs, Swapped: 0 reqs, Pending: 13105 reqs, GPU KV cache usage: 10.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:20 [metrics.py:486] Avg prompt throughput: 7579.9 tokens/s, Avg generation throughput: 1138.9 tokens/s, Running: 113 reqs, Swapped: 0 reqs, Pending: 13056 reqs, GPU KV cache usage: 12.2%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:25 [metrics.py:486] Avg prompt throughput: 8202.3 tokens/s, Avg generation throughput: 603.8 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 13005 reqs, GPU KV cache usage: 4.6%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:30 [metrics.py:486] Avg prompt throughput: 8896.9 tokens/s, Avg generation throughput: 263.2 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 12953 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:35 [metrics.py:486] Avg prompt throughput: 8632.4 tokens/s, Avg generation throughput: 268.6 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 12898 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:41 [metrics.py:486] Avg prompt throughput: 9227.4 tokens/s, Avg generation throughput: 252.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 12849 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:46 [metrics.py:486] Avg prompt throughput: 8861.2 tokens/s, Avg generation throughput: 310.1 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 12796 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:51 [metrics.py:486] Avg prompt throughput: 8918.0 tokens/s, Avg generation throughput: 264.5 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 12746 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.
INFO 07-22 16:02:56 [metrics.py:486] Avg prompt throughput: 9025.1 tokens/s, Avg generation throughput: 236.6 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 12695 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:01 [metrics.py:486] Avg prompt throughput: 8606.1 tokens/s, Avg generation throughput: 252.1 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 12639 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:06 [metrics.py:486] Avg prompt throughput: 8930.1 tokens/s, Avg generation throughput: 284.6 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 12587 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:11 [metrics.py:486] Avg prompt throughput: 8843.5 tokens/s, Avg generation throughput: 325.6 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 12534 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:16 [metrics.py:486] Avg prompt throughput: 9120.3 tokens/s, Avg generation throughput: 285.9 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 12485 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:21 [metrics.py:486] Avg prompt throughput: 8878.8 tokens/s, Avg generation throughput: 242.8 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 12432 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:26 [metrics.py:486] Avg prompt throughput: 8647.2 tokens/s, Avg generation throughput: 319.6 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 12377 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:31 [metrics.py:486] Avg prompt throughput: 8468.1 tokens/s, Avg generation throughput: 420.2 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 12321 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:36 [metrics.py:486] Avg prompt throughput: 8970.7 tokens/s, Avg generation throughput: 357.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 12271 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:41 [metrics.py:486] Avg prompt throughput: 9039.9 tokens/s, Avg generation throughput: 289.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 12220 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:46 [metrics.py:486] Avg prompt throughput: 9061.5 tokens/s, Avg generation throughput: 253.5 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 12168 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:51 [metrics.py:486] Avg prompt throughput: 9058.9 tokens/s, Avg generation throughput: 253.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 12116 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%.
INFO 07-22 16:03:56 [metrics.py:486] Avg prompt throughput: 9432.7 tokens/s, Avg generation throughput: 205.0 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 12068 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:01 [metrics.py:486] Avg prompt throughput: 8787.8 tokens/s, Avg generation throughput: 265.5 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 12012 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:07 [metrics.py:486] Avg prompt throughput: 8882.6 tokens/s, Avg generation throughput: 322.3 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 11958 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:12 [metrics.py:486] Avg prompt throughput: 8864.3 tokens/s, Avg generation throughput: 359.1 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 11906 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:17 [metrics.py:486] Avg prompt throughput: 8942.5 tokens/s, Avg generation throughput: 302.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 11855 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:22 [metrics.py:486] Avg prompt throughput: 8794.7 tokens/s, Avg generation throughput: 311.5 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 11802 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:27 [metrics.py:486] Avg prompt throughput: 8940.6 tokens/s, Avg generation throughput: 272.5 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 11750 reqs, GPU KV cache usage: 5.6%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:32 [metrics.py:486] Avg prompt throughput: 8880.1 tokens/s, Avg generation throughput: 288.6 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 11697 reqs, GPU KV cache usage: 6.5%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:37 [metrics.py:486] Avg prompt throughput: 9029.5 tokens/s, Avg generation throughput: 328.7 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 11647 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:42 [metrics.py:486] Avg prompt throughput: 8948.9 tokens/s, Avg generation throughput: 330.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 11597 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:47 [metrics.py:486] Avg prompt throughput: 8968.5 tokens/s, Avg generation throughput: 271.2 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 11547 reqs, GPU KV cache usage: 6.8%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:52 [metrics.py:486] Avg prompt throughput: 9242.6 tokens/s, Avg generation throughput: 228.1 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 11496 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.
INFO 07-22 16:04:57 [metrics.py:486] Avg prompt throughput: 9061.7 tokens/s, Avg generation throughput: 254.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 11445 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:02 [metrics.py:486] Avg prompt throughput: 9306.3 tokens/s, Avg generation throughput: 228.3 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 11395 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:07 [metrics.py:486] Avg prompt throughput: 8956.7 tokens/s, Avg generation throughput: 297.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 11342 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:12 [metrics.py:486] Avg prompt throughput: 8884.0 tokens/s, Avg generation throughput: 329.5 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 11290 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:17 [metrics.py:486] Avg prompt throughput: 9076.3 tokens/s, Avg generation throughput: 330.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 11241 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:23 [metrics.py:486] Avg prompt throughput: 8796.5 tokens/s, Avg generation throughput: 346.5 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 11188 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:28 [metrics.py:486] Avg prompt throughput: 9075.9 tokens/s, Avg generation throughput: 342.4 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 11139 reqs, GPU KV cache usage: 7.7%, CPU KV cache usage: 0.0%.
INFO 07-22 16:05:33 [metrics.py:486] Avg prompt throughput: 9205.8 tokens/s, Avg generation throughput: 350.6 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 11091 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%.
ERROR 07-22 16:05:43 [client.py:306] RuntimeError('Engine process (pid 210481) died.')
ERROR 07-22 16:05:43 [client.py:306] NoneType: None
INFO:     Shutting down
INFO:     Waiting for connections to close. (CTRL+C to force quit)
INFO:     127.0.0.1:50194 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [210183]
/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
