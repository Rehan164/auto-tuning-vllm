INFO 07-22 19:06:42 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 19:06:48 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 19:06:48 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 19:06:48 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 19:06:49 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 07-22 19:06:49 [cli_args.py:300] non-default args: {'block_size': 16, 'gpu_memory_utilization': 0.91, 'kv_cache_dtype': 'fp8', 'max_num_batched_tokens': 16384, 'max_num_partial_prefills': 8, 'cuda_graph_sizes': [968], 'long_prefill_token_threshold': 2048, 'enable_chunked_prefill': True, 'compilation_config': {"level": 3, "inductor_compile_config": {"enable_auto_functionalized_v2": false}}, 'disable_log_requests': True}
INFO 07-22 19:07:05 [config.py:793] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify', 'score'}. Defaulting to 'generate'.
WARNING 07-22 19:07:05 [arg_utils.py:1583] --kv-cache-dtype is not supported by the V1 Engine. Falling back to V0. 
INFO 07-22 19:07:05 [config.py:1503] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-22 19:07:05 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 07-22 19:07:05 [config.py:2128] Concurrent partial prefills enabled with max_num_partial_prefills=8, max_long_partial_prefills=1, long_prefill_token_threshold=2048
INFO 07-22 19:07:05 [api_server.py:257] Started engine process with PID 220382
INFO 07-22 19:07:08 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 19:07:11 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 19:07:11 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 19:07:11 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 19:07:11 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 256}, use_cached_outputs=True, 
INFO 07-22 19:07:18 [cuda.py:273] Cannot use FlashAttention backend for FP8 KV cache.
WARNING 07-22 19:07:18 [cuda.py:275] Please use FlashInfer backend with FP8 KV Cache for better performance by setting environment variable VLLM_ATTENTION_BACKEND=FLASHINFER
INFO 07-22 19:07:18 [cuda.py:289] Using XFormers backend.
INFO 07-22 19:07:19 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-22 19:07:19 [model_runner.py:1170] Starting to load model meta-llama/Llama-3.1-8B...
INFO 07-22 19:07:19 [backends.py:35] Using InductorAdaptor
INFO 07-22 19:07:20 [weight_utils.py:291] Using model weights format ['*.safetensors']
INFO 07-22 19:07:21 [weight_utils.py:307] Time spent downloading weights for meta-llama/Llama-3.1-8B: 0.587306 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.72it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.72it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.57it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]

INFO 07-22 19:07:24 [default_loader.py:280] Loading weights took 2.33 seconds
INFO 07-22 19:07:24 [model_runner.py:1202] Model loading took 14.9889 GiB and 5.079960 seconds
INFO 07-22 19:07:30 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/5750ec20ff/rank_0_0 for vLLM's torch.compile
INFO 07-22 19:07:30 [backends.py:469] Dynamo bytecode transform time: 5.70 s
INFO 07-22 19:07:33 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 0.119 s
INFO 07-22 19:07:33 [monitor.py:33] torch.compile takes 5.70 s in total
INFO 07-22 19:07:35 [worker.py:291] Memory profiling takes 10.48 seconds
INFO 07-22 19:07:35 [worker.py:291] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.91) = 40.51GiB
INFO 07-22 19:07:35 [worker.py:291] model weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.54GiB; the rest of the memory reserved for KV Cache is 22.90GiB.
INFO 07-22 19:07:35 [executor_base.py:112] # cuda blocks: 23450, # CPU blocks: 4096
INFO 07-22 19:07:35 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 2.86x
INFO 07-22 19:07:36 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:18,  1.79it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:16,  1.96it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:15,  2.02it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:15,  2.06it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:14,  2.06it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.10it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:13,  2.12it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.14it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:12,  2.15it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.16it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:11,  2.10it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.13it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:10,  2.14it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.14it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:07<00:09,  2.09it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:09,  2.10it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:08<00:08,  2.10it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:08,  2.08it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:09<00:07,  2.10it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:07,  2.07it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:10<00:06,  2.11it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:06,  2.13it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.07it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:11<00:05,  2.10it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.10it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:12<00:04,  2.09it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.10it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:13<00:03,  2.06it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.12it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:14<00:02,  2.13it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.11it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:15<00:01,  2.15it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.13it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:16<00:00,  2.16it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.15it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]
INFO 07-22 19:07:53 [model_runner.py:1670] Graph capturing finished in 17 secs, took 0.27 GiB
INFO 07-22 19:07:53 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 28.68 seconds
WARNING 07-22 19:07:54 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 07-22 19:07:54 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 19:07:54 [serving_completion.py:65] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 19:07:54 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000
INFO 07-22 19:07:54 [launcher.py:28] Available routes are:
INFO 07-22 19:07:54 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /health, Methods: GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /load, Methods: GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /ping, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /ping, Methods: GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /version, Methods: GET
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /pooling, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /classify, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /score, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /rerank, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /invocations, Methods: POST
INFO 07-22 19:07:54 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [220092]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:55328 - "GET /health HTTP/1.1" 200 OK
INFO 07-22 19:09:18 [launcher.py:79] Shutting down FastAPI HTTP server.
Exception ignored in atexit callback: <function shutdown at 0x7f5b0b4d44a0>
Traceback (most recent call last):
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/_private/worker.py", line 1950, in shutdown
    from ray.dag.compiled_dag_node import _shutdown_all_compiled_dags
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/dag/__init__.py", line 1, in <module>
    from ray.dag.dag_node import DAGNode
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/dag/dag_node.py", line 2, in <module>
    from ray.experimental.channel.auto_transport_type import AutoTransportType
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/experimental/channel/__init__.py", line 1, in <module>
    from ray.experimental.channel.cached_channel import CachedChannel
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/experimental/channel/cached_channel.py", line 4, in <module>
    from ray.experimental.channel.common import ChannelInterface
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/experimental/channel/common.py", line 175, in <module>
    class ChannelInterface:
  File "/root/rehan/.venv/lib/python3.12/site-packages/ray/experimental/channel/common.py", line 183, in ChannelInterface
    writer: Optional[ray.actor.ActorHandle],
            ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 395, in inner
    return _caches[func](*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 517, in __getitem__
    return self._getitem(self, parameters)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 751, in Optional
    return Union[arg, type(None)]
           ~~~~~^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 395, in inner
    return _caches[func](*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 517, in __getitem__
    return self._getitem(self, parameters)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 731, in Union
    parameters = _remove_dups_flatten(parameters)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 358, in _remove_dups_flatten
    return tuple(_deduplicate(params, unhashable_fallback=True))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/typing.py", line 320, in _deduplicate
    return dict.fromkeys(params)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 435, in signal_handler
    raise KeyboardInterrupt("MQLLMEngine terminated")
KeyboardInterrupt: MQLLMEngine terminated
[rank0]:[W722 19:09:18.297975522 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
