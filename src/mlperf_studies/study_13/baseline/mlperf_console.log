INFO 07-22 20:35:57 [__init__.py:243] Automatically detected platform cuda.
/root/rehan/mlperf-inference-5.1-redhat/language/llama3.1-8b/SUT_VLLM_SingleReplica.py:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  import pkg_resources
================================================================================
COMMAND LINE AND EXECUTABLE INFORMATION
================================================================================
Executable: /root/rehan/.venv/bin/python3
Command line: SUT_VLLM_SingleReplica.py --model_name meta-llama/Llama-3.1-8B --batch_size 32 --max_model_len 8192 --num_gpus 1 --output-log-dir /root/rehan/src/mlperf_studies/study_13/baseline --dataset_path /root/rehan/datasets/cnn_eval.json
================================================================================
Current date and time: 2025-07-22 20:35:59
================================================================================
Installed Python packages:
  aiohappyeyeballs               2.6.1
  aiohttp                        3.12.7
  aiosignal                      1.3.2
  airportsdata                   20250523
  alembic                        1.16.1
  annotated-types                0.7.0
  anyio                          4.9.0
  astor                          0.8.1
  attrs                          25.3.0
  autocommand                    2.2.2
  backports.tarfile              1.2.0
  blake3                         1.0.5
  botorch                        0.9.5
  bottle                         0.13.3
  cachetools                     6.0.0
  certifi                        2025.4.26
  charset-normalizer             3.4.2
  choreographer                  1.0.9
  click                          8.1.8
  cloudpickle                    3.1.1
  colorama                       0.4.6
  colorlog                       6.9.0
  compressed-tensors             0.9.4
  contourpy                      1.3.2
  cupy-cuda12x                   13.4.1
  cycler                         0.12.1
  datasets                       3.6.0
  Deprecated                     1.2.18
  depyf                          0.18.0
  dill                           0.3.8
  diskcache                      5.6.3
  distro                         1.9.0
  dnspython                      2.7.0
  einops                         0.8.1
  email-validator                2.2.0
  fastapi                        0.115.12
  fastapi-cli                    0.0.7
  fastrlock                      0.8.3
  filelock                       3.18.0
  fonttools                      4.58.4
  frozenlist                     1.6.0
  fsspec                         2025.3.0
  ftfy                           6.3.1
  gguf                           0.17.0
  giturlparse                    0.12.0
  googleapis-common-protos       1.70.0
  gpytorch                       1.11
  greenlet                       3.2.2
  grpcio                         1.72.1
  guidellm                       0.1.0.dev0
  h11                            0.16.0
  h2                             4.2.0
  hf-xet                         1.1.2
  hpack                          4.1.0
  httpcore                       1.0.9
  httptools                      0.6.4
  httpx                          0.28.1
  huggingface-hub                0.32.4
  hyperframe                     6.1.0
  idna                           3.10
  importlib-metadata             8.6.1
  inflect                        7.3.1
  interegular                    0.3.3
  jaraco.collections             5.1.0
  jaraco.context                 5.3.0
  jaraco.functools               4.0.1
  jaraco.text                    3.12.1
  jaxtyping                      0.3.2
  jinja2                         3.1.6
  jiter                          0.10.0
  joblib                         1.5.1
  jsonschema                     4.24.0
  jsonschema-specifications      2025.4.1
  kaleido                        1.0.0
  kiwisolver                     1.4.8
  lark                           1.2.2
  linear-operator                0.5.1
  llguidance                     0.7.26
  llvmlite                       0.44.0
  lm-format-enforcer             0.10.11
  logistro                       1.1.0
  loguru                         0.7.3
  mako                           1.3.10
  markdown-it-py                 3.0.0
  MarkupSafe                     3.0.2
  matplotlib                     3.10.3
  mdurl                          0.1.2
  mistral-common                 1.5.6
  mlc-scripts                    1.0.2
  mlcflow                        1.0.18
  mlcommons-loadgen              5.0.25
  more-itertools                 10.3.0
  mpmath                         1.3.0
  msgpack                        1.1.0
  msgspec                        0.19.0
  multidict                      6.4.4
  multipledispatch               1.0.0
  multiprocess                   0.70.16
  mypy-extensions                1.1.0
  narwhals                       1.44.0
  nest-asyncio                   1.6.0
  networkx                       3.5
  ninja                          1.11.1.4
  numba                          0.61.2
  numpy                          2.2.6
  nvidia-cublas-cu12             12.6.4.1
  nvidia-cuda-cupti-cu12         12.6.80
  nvidia-cuda-nvrtc-cu12         12.6.77
  nvidia-cuda-runtime-cu12       12.6.77
  nvidia-cudnn-cu12              9.5.1.17
  nvidia-cufft-cu12              11.3.0.4
  nvidia-cufile-cu12             1.11.1.6
  nvidia-curand-cu12             10.3.7.77
  nvidia-cusolver-cu12           11.7.1.2
  nvidia-cusparse-cu12           12.5.4.2
  nvidia-cusparselt-cu12         0.6.3
  nvidia-nccl-cu12               2.26.2
  nvidia-nvjitlink-cu12          12.6.85
  nvidia-nvtx-cu12               12.6.77
  openai                         1.83.0
  opencv-python-headless         4.11.0.86
  opentelemetry-api              1.33.1
  opentelemetry-exporter-otlp    1.33.1
  opentelemetry-exporter-otlp-proto-common 1.33.1
  opentelemetry-exporter-otlp-proto-grpc 1.33.1
  opentelemetry-exporter-otlp-proto-http 1.33.1
  opentelemetry-proto            1.33.1
  opentelemetry-sdk              1.33.1
  opentelemetry-semantic-conventions 0.54b1
  opentelemetry-semantic-conventions-ai 0.4.9
  opt-einsum                     3.4.0
  optuna                         4.4.0
  optuna-dashboard               0.18.0
  optuna-integration             4.4.0
  orjson                         3.10.18
  outlines                       0.1.11
  outlines-core                  0.1.26
  packaging                      25.0
  panda                          0.3.1
  pandas                         2.3.0
  partial-json-parser            0.2.1.1.post5
  pillow                         11.2.1
  pip                            25.1.1
  platformdirs                   4.2.2
  plotly                         6.1.2
  prometheus-client              0.22.1
  prometheus-fastapi-instrumentator 7.1.0
  propcache                      0.3.1
  protobuf                       5.29.5
  psutil                         7.0.0
  py-cpuinfo                     9.0.0
  pyarrow                        20.0.0
  pybind11                       3.0.0
  pycountry                      24.6.1
  pydantic                       2.11.7
  pydantic-core                  2.33.2
  pydantic-settings              2.9.1
  pygments                       2.19.1
  pyparsing                      3.2.3
  pyre-extensions                0.0.32
  pyro-api                       0.1.2
  pyro-ppl                       1.9.1
  python-dateutil                2.9.0.post0
  python-dotenv                  1.1.0
  python-json-logger             3.3.0
  python-multipart               0.0.20
  pytz                           2025.2
  PyYAML                         6.0.2
  pyzmq                          26.4.0
  ray                            2.46.0
  referencing                    0.36.2
  regex                          2024.11.6
  requests                       2.32.3
  rich                           14.0.0
  rich-toolkit                   0.14.7
  rpds-py                        0.25.1
  safetensors                    0.5.3
  scikit-learn                   1.7.0
  scipy                          1.15.3
  seaborn                        0.13.2
  sentencepiece                  0.2.0
  setproctitle                   1.2.2
  setuptools                     79.0.1
  shellingham                    1.5.4
  simplejson                     3.20.1
  six                            1.17.0
  sniffio                        1.3.1INFO:root:--------------------------------------------------
INFO:root:Using local vLLM model
INFO:Llama-8B-Dataset:Loading dataset...
INFO:Llama-8B-Dataset:Finished loading dataset.
INFO:root:Dataset Max          = 2540
INFO:root:Dataset Min          = 79
INFO:root:Dataset TotalSamples = 13368
INFO:root:Loading model 'meta-llama/Llama-3.1-8B' on single GPU...

  sqlalchemy                     2.0.41
  starlette                      0.46.2
  sympy                          1.14.0
  threadpoolctl                  3.6.0
  tiktoken                       0.9.0
  tokenizers                     0.21.1
  tomli                          2.0.1
  torch                          2.7.0
  torchaudio                     2.7.0
  torchvision                    0.22.0
  tqdm                           4.67.1
  transformers                   4.52.4
  triton                         3.3.0
  typeguard                      2.13.3
  typer                          0.16.0
  typing-extensions              4.14.0
  typing-inspect                 0.9.0
  typing-inspection              0.4.1
  tzdata                         2025.2
  urllib3                        2.4.0
  uvicorn                        0.34.3
  uvloop                         0.21.0
  vllm                           0.9.0.1
  wadler-lindig                  0.1.7
  watchfiles                     1.0.5
  wcwidth                        0.2.13
  websockets                     15.0.1
  wheel                          0.45.1
  wrapt                          1.17.2
  xformers                       0.0.30
  xgrammar                       0.1.19
  xxhash                         3.5.0
  yarl                           1.20.0
  zipp                           3.22.0
================================================================================

Set TORCH_CUDA_ARCH_LIST to 8.9
INFO 07-22 20:36:00 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 20:36:00 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 20:36:00 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 20:36:16 [config.py:793] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 07-22 20:36:16 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 07-22 20:36:20 [utils.py:2531] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 07-22 20:36:23 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 20:36:26 [core.py:438] Waiting for init message from front-end.
INFO 07-22 20:36:26 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 20:36:26 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 20:36:26 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 20:36:26 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 07-22 20:36:26 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2cee7fe150>
INFO 07-22 20:36:31 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-22 20:36:31 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-22 20:36:31 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.1-8B...
INFO 07-22 20:36:31 [cuda.py:217] Using Flash Attention backend on V1 engine.
ERROR 07-22 20:36:31 [core.py:500] EngineCore failed to start.
ERROR 07-22 20:36:31 [core.py:500] Traceback (most recent call last):
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 07-22 20:36:31 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-22 20:36:31 [core.py:500]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 07-22 20:36:31 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 71, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.model_executor = executor_class(vllm_config)
ERROR 07-22 20:36:31 [core.py:500]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self._init_executor()
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 07-22 20:36:31 [core.py:500]     self.collective_rpc("load_model")
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 07-22 20:36:31 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 07-22 20:36:31 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
ERROR 07-22 20:36:31 [core.py:500]     return func(*args, **kwargs)
ERROR 07-22 20:36:31 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
ERROR 07-22 20:36:31 [core.py:500]     self.model_runner.load_model()
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
ERROR 07-22 20:36:31 [core.py:500]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 07-22 20:36:31 [core.py:500]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
ERROR 07-22 20:36:31 [core.py:500]     return loader.load_model(vllm_config=vllm_config,
ERROR 07-22 20:36:31 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
ERROR 07-22 20:36:31 [core.py:500]     model = initialize_model(vllm_config=vllm_config,
ERROR 07-22 20:36:31 [core.py:500]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
ERROR 07-22 20:36:31 [core.py:500]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 07-22 20:36:31 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 520, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 07-22 20:36:31 [core.py:500]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 566, in _init_model
ERROR 07-22 20:36:31 [core.py:500]     return LlamaModel(vllm_config=vllm_config,
ERROR 07-22 20:36:31 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 07-22 20:36:31 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 345, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 07-22 20:36:31 [core.py:500]                                                     ^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
ERROR 07-22 20:36:31 [core.py:500]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 07-22 20:36:31 [core.py:500]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 347, in <lambda>
ERROR 07-22 20:36:31 [core.py:500]     lambda prefix: layer_type(config=config,
ERROR 07-22 20:36:31 [core.py:500]                    ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 278, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.mlp = LlamaMLP(
ERROR 07-22 20:36:31 [core.py:500]                ^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 78, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.down_proj = RowParallelLinear(
ERROR 07-22 20:36:31 [core.py:500]                      ^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
ERROR 07-22 20:36:31 [core.py:500]     self.quant_method.create_weights(
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 07-22 20:36:31 [core.py:500]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 07-22 20:36:31 [core.py:500]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 07-22 20:36:31 [core.py:500]     return func(*args, **kwargs)
ERROR 07-22 20:36:31 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 07-22 20:36:31 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 10.50 MiB is free. Process 226852 has 40.60 GiB memory in use. Including non-PyTorch memory, this process has 3.90 GiB memory in use. Of the allocated memory 3.40 GiB is allocated by PyTorch, and 16.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 71, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
    self.model_runner.load_model()
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
    return loader.load_model(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
    model = initialize_model(vllm_config=vllm_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 520, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 566, in _init_model
    return LlamaModel(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 345, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
                                                    ^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 347, in <lambda>
    lambda prefix: layer_type(config=config,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 278, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 78, in __init__
    self.down_proj = RowParallelLinear(
                     ^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
    self.quant_method.create_weights(
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 10.50 MiB is free. Process 226852 has 40.60 GiB memory in use. Including non-PyTorch memory, this process has 3.90 GiB memory in use. Of the allocated memory 3.40 GiB is allocated by PyTorch, and 16.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W722 20:36:32.778877605 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
CRITICAL:root:
Main program encountered an error: Engine core initialization failed. See root cause above. Failed core proc(s): {}
