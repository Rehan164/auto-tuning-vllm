INFO 07-22 20:36:43 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 20:36:50 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 20:36:50 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 20:36:50 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 20:36:51 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 07-22 20:36:51 [cli_args.py:300] non-default args: {'block_size': 16, 'kv_cache_dtype': 'fp8', 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_num_partial_prefills': 4, 'cuda_graph_sizes': [4680], 'long_prefill_token_threshold': 1024, 'enable_chunked_prefill': True, 'compilation_config': {"level": 3, "inductor_compile_config": {"enable_auto_functionalized_v2": false}}, 'disable_log_requests': True}
INFO 07-22 20:37:01 [config.py:793] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
WARNING 07-22 20:37:01 [arg_utils.py:1583] --kv-cache-dtype is not supported by the V1 Engine. Falling back to V0. 
INFO 07-22 20:37:01 [config.py:1503] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-22 20:37:01 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 07-22 20:37:01 [config.py:2128] Concurrent partial prefills enabled with max_num_partial_prefills=4, max_long_partial_prefills=1, long_prefill_token_threshold=1024
INFO 07-22 20:37:01 [api_server.py:257] Started engine process with PID 227717
INFO 07-22 20:37:05 [__init__.py:243] Automatically detected platform cuda.
INFO 07-22 20:37:08 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 07-22 20:37:08 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 07-22 20:37:08 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-22 20:37:08 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 128}, use_cached_outputs=True, 
INFO 07-22 20:37:10 [cuda.py:273] Cannot use FlashAttention backend for FP8 KV cache.
WARNING 07-22 20:37:10 [cuda.py:275] Please use FlashInfer backend with FP8 KV Cache for better performance by setting environment variable VLLM_ATTENTION_BACKEND=FLASHINFER
INFO 07-22 20:37:10 [cuda.py:289] Using XFormers backend.
INFO 07-22 20:37:16 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-22 20:37:16 [model_runner.py:1170] Starting to load model meta-llama/Llama-3.1-8B...
INFO 07-22 20:37:16 [backends.py:35] Using InductorAdaptor
INFO 07-22 20:37:17 [weight_utils.py:291] Using model weights format ['*.safetensors']
INFO 07-22 20:37:18 [weight_utils.py:307] Time spent downloading weights for meta-llama/Llama-3.1-8B: 0.617637 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.66it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.14it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.71it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]

INFO 07-22 20:37:21 [default_loader.py:280] Loading weights took 2.34 seconds
INFO 07-22 20:37:21 [model_runner.py:1202] Model loading took 14.9889 GiB and 5.027402 seconds
INFO 07-22 20:37:27 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/5750ec20ff/rank_0_0 for vLLM's torch.compile
INFO 07-22 20:37:27 [backends.py:469] Dynamo bytecode transform time: 5.73 s
INFO 07-22 20:37:29 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 0.119 s
INFO 07-22 20:37:30 [monitor.py:33] torch.compile takes 5.73 s in total
INFO 07-22 20:37:31 [worker.py:291] Memory profiling takes 9.87 seconds
INFO 07-22 20:37:31 [worker.py:291] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.90) = 40.07GiB
INFO 07-22 20:37:31 [worker.py:291] model weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.27GiB; the rest of the memory reserved for KV Cache is 23.72GiB.
INFO 07-22 20:37:31 [executor_base.py:112] # cuda blocks: 24292, # CPU blocks: 4096
INFO 07-22 20:37:31 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 2.97x
INFO 07-22 20:37:32 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graph shapes:   5%|▌         | 1/19 [00:00<00:10,  1.79it/s]Capturing CUDA graph shapes:  11%|█         | 2/19 [00:01<00:09,  1.86it/s]Capturing CUDA graph shapes:  16%|█▌        | 3/19 [00:01<00:08,  1.95it/s]Capturing CUDA graph shapes:  21%|██        | 4/19 [00:02<00:07,  2.02it/s]Capturing CUDA graph shapes:  26%|██▋       | 5/19 [00:02<00:06,  2.04it/s]Capturing CUDA graph shapes:  32%|███▏      | 6/19 [00:02<00:06,  2.09it/s]Capturing CUDA graph shapes:  37%|███▋      | 7/19 [00:03<00:06,  1.89it/s]Capturing CUDA graph shapes:  42%|████▏     | 8/19 [00:04<00:05,  1.98it/s]Capturing CUDA graph shapes:  47%|████▋     | 9/19 [00:04<00:05,  1.80it/s]Capturing CUDA graph shapes:  53%|█████▎    | 10/19 [00:05<00:04,  1.91it/s]Capturing CUDA graph shapes:  58%|█████▊    | 11/19 [00:05<00:04,  1.92it/s]Capturing CUDA graph shapes:  63%|██████▎   | 12/19 [00:06<00:03,  1.96it/s]Capturing CUDA graph shapes:  68%|██████▊   | 13/19 [00:06<00:03,  2.00it/s]Capturing CUDA graph shapes:  74%|███████▎  | 14/19 [00:07<00:02,  2.02it/s]Capturing CUDA graph shapes:  79%|███████▉  | 15/19 [00:07<00:01,  2.03it/s]Capturing CUDA graph shapes:  84%|████████▍ | 16/19 [00:08<00:01,  2.06it/s]Capturing CUDA graph shapes:  89%|████████▉ | 17/19 [00:08<00:00,  2.09it/s]Capturing CUDA graph shapes:  95%|█████████▍| 18/19 [00:09<00:00,  2.12it/s]Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:09<00:00,  2.10it/s]Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:09<00:00,  2.00it/s]
INFO 07-22 20:37:42 [model_runner.py:1670] Graph capturing finished in 10 secs, took 0.16 GiB
INFO 07-22 20:37:42 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 20.95 seconds
WARNING 07-22 20:37:43 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 07-22 20:37:43 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 20:37:43 [serving_completion.py:65] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-22 20:37:43 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000
INFO 07-22 20:37:43 [launcher.py:28] Available routes are:
INFO 07-22 20:37:43 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 07-22 20:37:43 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 07-22 20:37:43 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 07-22 20:37:43 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 07-22 20:37:43 [launcher.py:36] Route: /health, Methods: GET
INFO 07-22 20:37:43 [launcher.py:36] Route: /load, Methods: GET
INFO 07-22 20:37:43 [launcher.py:36] Route: /ping, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /ping, Methods: GET
INFO 07-22 20:37:43 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 07-22 20:37:43 [launcher.py:36] Route: /version, Methods: GET
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /pooling, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /classify, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /score, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /rerank, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /invocations, Methods: POST
INFO 07-22 20:37:43 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [227415]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
ERROR 07-22 20:37:53 [client.py:306] RuntimeError('Engine process (pid 227717) died.')
ERROR 07-22 20:37:53 [client.py:306] NoneType: None
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [227415]
/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
