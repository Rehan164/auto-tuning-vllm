INFO 06-17 15:44:14 [__init__.py:243] Automatically detected platform cuda.
INFO 06-17 15:44:16 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-17 15:44:16 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-17 15:44:16 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-17 15:44:17 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 06-17 15:44:18 [cli_args.py:300] non-default args: {'max_model_len': 8192, 'disable_log_requests': True}
INFO 06-17 15:44:28 [config.py:793] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
INFO 06-17 15:44:28 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 06-17 15:44:33 [__init__.py:243] Automatically detected platform cuda.
INFO 06-17 15:44:36 [core.py:438] Waiting for init message from front-end.
INFO 06-17 15:44:36 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-17 15:44:36 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-17 15:44:36 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-17 15:44:36 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='Qwen/Qwen3-32B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-32B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-32B-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-17 15:44:36 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7efebea1b740>
INFO 06-17 15:44:41 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 06-17 15:44:41 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-17 15:44:42 [gpu_model_runner.py:1531] Starting to load model Qwen/Qwen3-32B-FP8...
INFO 06-17 15:44:42 [cuda.py:217] Using Flash Attention backend on V1 engine.
ERROR 06-17 15:44:42 [core.py:500] EngineCore failed to start.
ERROR 06-17 15:44:42 [core.py:500] Traceback (most recent call last):
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 06-17 15:44:42 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-17 15:44:42 [core.py:500]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 06-17 15:44:42 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 71, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.model_executor = executor_class(vllm_config)
ERROR 06-17 15:44:42 [core.py:500]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self._init_executor()
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 06-17 15:44:42 [core.py:500]     self.collective_rpc("load_model")
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 06-17 15:44:42 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-17 15:44:42 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
ERROR 06-17 15:44:42 [core.py:500]     return func(*args, **kwargs)
ERROR 06-17 15:44:42 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
ERROR 06-17 15:44:42 [core.py:500]     self.model_runner.load_model()
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
ERROR 06-17 15:44:42 [core.py:500]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 06-17 15:44:42 [core.py:500]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
ERROR 06-17 15:44:42 [core.py:500]     return loader.load_model(vllm_config=vllm_config,
ERROR 06-17 15:44:42 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
ERROR 06-17 15:44:42 [core.py:500]     model = initialize_model(vllm_config=vllm_config,
ERROR 06-17 15:44:42 [core.py:500]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
ERROR 06-17 15:44:42 [core.py:500]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 06-17 15:44:42 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 270, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.model = Qwen3Model(vllm_config=vllm_config,
ERROR 06-17 15:44:42 [core.py:500]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 06-17 15:44:42 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 242, in __init__
ERROR 06-17 15:44:42 [core.py:500]     super().__init__(vllm_config=vllm_config,
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 06-17 15:44:42 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 320, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 06-17 15:44:42 [core.py:500]                                                     ^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
ERROR 06-17 15:44:42 [core.py:500]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 06-17 15:44:42 [core.py:500]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 322, in <lambda>
ERROR 06-17 15:44:42 [core.py:500]     lambda prefix: decoder_layer_type(config=config,
ERROR 06-17 15:44:42 [core.py:500]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 188, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.mlp = Qwen3MLP(
ERROR 06-17 15:44:42 [core.py:500]                ^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 82, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.down_proj = RowParallelLinear(
ERROR 06-17 15:44:42 [core.py:500]                      ^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
ERROR 06-17 15:44:42 [core.py:500]     self.quant_method.create_weights(
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/fp8.py", line 248, in create_weights
ERROR 06-17 15:44:42 [core.py:500]     weight = ModelWeightParameter(data=torch.empty(
ERROR 06-17 15:44:42 [core.py:500]                                        ^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 06-17 15:44:42 [core.py:500]     return func(*args, **kwargs)
ERROR 06-17 15:44:42 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-17 15:44:42 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 76.50 MiB is free. Process 66192 has 41.23 GiB memory in use. Including non-PyTorch memory, this process has 3.20 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 1.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 71, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
    self.model_runner.load_model()
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
    return loader.load_model(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
    model = initialize_model(vllm_config=vllm_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 270, in __init__
    self.model = Qwen3Model(vllm_config=vllm_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 242, in __init__
    super().__init__(vllm_config=vllm_config,
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 320, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
                                                    ^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 322, in <lambda>
    lambda prefix: decoder_layer_type(config=config,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 188, in __init__
    self.mlp = Qwen3MLP(
               ^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 82, in __init__
    self.down_proj = RowParallelLinear(
                     ^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
    self.quant_method.create_weights(
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/fp8.py", line 248, in create_weights
    weight = ModelWeightParameter(data=torch.empty(
                                       ^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 76.50 MiB is free. Process 66192 has 41.23 GiB memory in use. Including non-PyTorch memory, this process has 3.20 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 1.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W617 15:44:42.133450545 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
