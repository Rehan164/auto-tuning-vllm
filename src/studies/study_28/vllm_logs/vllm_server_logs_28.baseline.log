INFO 06-16 18:50:44 [__init__.py:243] Automatically detected platform cuda.
INFO 06-16 18:50:47 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-16 18:50:47 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-16 18:50:47 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-16 18:50:48 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 06-16 18:50:48 [cli_args.py:300] non-default args: {'max_model_len': 4096, 'disable_log_requests': True}
INFO 06-16 18:51:01 [config.py:793] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 06-16 18:51:01 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 06-16 18:51:05 [__init__.py:243] Automatically detected platform cuda.
INFO 06-16 18:51:08 [core.py:438] Waiting for init message from front-end.
INFO 06-16 18:51:08 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-16 18:51:08 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-16 18:51:08 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-16 18:51:08 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-1.7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-16 18:51:08 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fdb369e8230>
INFO 06-16 18:51:14 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 06-16 18:51:14 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-16 18:51:14 [gpu_model_runner.py:1531] Starting to load model Qwen/Qwen3-1.7B...
INFO 06-16 18:51:14 [cuda.py:217] Using Flash Attention backend on V1 engine.
INFO 06-16 18:51:14 [backends.py:35] Using InductorAdaptor
INFO 06-16 18:51:14 [backends.py:35] Using InductorAdaptor
INFO 06-16 18:51:14 [weight_utils.py:291] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.78it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.78it/s]

INFO 06-16 18:51:15 [default_loader.py:280] Loading weights took 0.57 seconds
INFO 06-16 18:51:15 [gpu_model_runner.py:1549] Model loading took 3.2152 GiB and 1.293451 seconds
INFO 06-16 18:51:21 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/399cc71d00/rank_0_0 for vLLM's torch.compile
INFO 06-16 18:51:21 [backends.py:469] Dynamo bytecode transform time: 6.32 s
INFO 06-16 18:51:26 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 4.274 s
INFO 06-16 18:51:27 [monitor.py:33] torch.compile takes 6.32 s in total
ERROR 06-16 18:51:27 [core.py:500] EngineCore failed to start.
ERROR 06-16 18:51:27 [core.py:500] Traceback (most recent call last):
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1787, in _dummy_sampler_run
ERROR 06-16 18:51:27 [core.py:500]     sampler_output = self.sampler(logits=logits,
ERROR 06-16 18:51:27 [core.py:500]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 06-16 18:51:27 [core.py:500]     return self._call_impl(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 06-16 18:51:27 [core.py:500]     return forward_call(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
ERROR 06-16 18:51:27 [core.py:500]     sampled = self.sample(logits, sampling_metadata)
ERROR 06-16 18:51:27 [core.py:500]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
ERROR 06-16 18:51:27 [core.py:500]     random_sampled = self.topk_topp_sampler(
ERROR 06-16 18:51:27 [core.py:500]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 06-16 18:51:27 [core.py:500]     return self._call_impl(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 06-16 18:51:27 [core.py:500]     return forward_call(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 80, in forward_native
ERROR 06-16 18:51:27 [core.py:500]     logits = apply_top_k_top_p(logits, k, p)
ERROR 06-16 18:51:27 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 183, in apply_top_k_top_p
ERROR 06-16 18:51:27 [core.py:500]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
ERROR 06-16 18:51:27 [core.py:500]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 114.50 MiB is free. Process 18961 has 39.93 GiB memory in use. Including non-PyTorch memory, this process has 4.47 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 91.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 06-16 18:51:27 [core.py:500] 
ERROR 06-16 18:51:27 [core.py:500] The above exception was the direct cause of the following exception:
ERROR 06-16 18:51:27 [core.py:500] 
ERROR 06-16 18:51:27 [core.py:500] Traceback (most recent call last):
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 06-16 18:51:27 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 06-16 18:51:27 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 78, in __init__
ERROR 06-16 18:51:27 [core.py:500]     self._initialize_kv_caches(vllm_config)
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 137, in _initialize_kv_caches
ERROR 06-16 18:51:27 [core.py:500]     available_gpu_memory = self.model_executor.determine_available_memory()
ERROR 06-16 18:51:27 [core.py:500]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 75, in determine_available_memory
ERROR 06-16 18:51:27 [core.py:500]     output = self.collective_rpc("determine_available_memory")
ERROR 06-16 18:51:27 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 06-16 18:51:27 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-16 18:51:27 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
ERROR 06-16 18:51:27 [core.py:500]     return func(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 06-16 18:51:27 [core.py:500]     return func(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in determine_available_memory
ERROR 06-16 18:51:27 [core.py:500]     self.model_runner.profile_run()
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1899, in profile_run
ERROR 06-16 18:51:27 [core.py:500]     sampler_output = self._dummy_sampler_run(hidden_states)
ERROR 06-16 18:51:27 [core.py:500]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 06-16 18:51:27 [core.py:500]     return func(*args, **kwargs)
ERROR 06-16 18:51:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-16 18:51:27 [core.py:500]   File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1791, in _dummy_sampler_run
ERROR 06-16 18:51:27 [core.py:500]     raise RuntimeError(
ERROR 06-16 18:51:27 [core.py:500] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
Process EngineCore_0:
Traceback (most recent call last):
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1787, in _dummy_sampler_run
    sampler_output = self.sampler(logits=logits,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
    sampled = self.sample(logits, sampling_metadata)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
    random_sampled = self.topk_topp_sampler(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 80, in forward_native
    logits = apply_top_k_top_p(logits, k, p)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 183, in apply_top_k_top_p
    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 114.50 MiB is free. Process 18961 has 39.93 GiB memory in use. Including non-PyTorch memory, this process has 4.47 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 91.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 78, in __init__
    self._initialize_kv_caches(vllm_config)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 137, in _initialize_kv_caches
    available_gpu_memory = self.model_executor.determine_available_memory()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 75, in determine_available_memory
    output = self.collective_rpc("determine_available_memory")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/utils.py", line 2605, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in determine_available_memory
    self.model_runner.profile_run()
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1899, in profile_run
    sampler_output = self._dummy_sampler_run(hidden_states)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1791, in _dummy_sampler_run
    raise RuntimeError(
RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W616 18:51:27.358366806 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/root/rehan/.venv/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 56, in main
    args.dispatch_function(args)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 42, in cmd
    uvloop.run(run_server(args))
  File "/root/rehan/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/root/rehan/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1324, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 153, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 185, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 157, in from_vllm_config
    return cls(
           ^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 123, in __init__
    self.engine_core = core_client_class(
                       ^^^^^^^^^^^^^^^^^^
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 734, in __init__
    super().__init__(
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 418, in __init__
    self._wait_for_engine_startup(output_address, parallel_config)
  File "/root/rehan/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 484, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
